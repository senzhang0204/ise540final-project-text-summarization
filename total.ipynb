{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bce0c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import nltk\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b1b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc50fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_path='F:/Course/2021 Fall/ISE 540/Project/book/history' # book directory\n",
    "book_list=os.listdir(input_path)                              # book list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa542e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.DataFrame(columns=['tf_sum','tf_len','optimal_cut'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d230272b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "done=[]\n",
    "wrong=[]\n",
    "for books in book_list: \n",
    "    try:\n",
    "        input_file=input_path+'/'+books\n",
    "\n",
    "        def read_article(file_name):\n",
    "            file = open(file_name, \"r\", encoding=\"UTF-8\")\n",
    "            filedata = file.readlines()\n",
    "            # print(filedata)\n",
    "\n",
    "            filedata_new = \" \".join(filedata)               # join every elements in the list into a single element\n",
    "            paragraph = filedata_new.replace(\"\\n \\n \", \".\") # replace newlines between paragraphs by periods(there are usually 2 or 3 newlines between paragraphs, so replace by at least 2 newlines)\n",
    "            paragraph = paragraph.split(\".\")                # Split all periods, so we get sentence by sentence\n",
    "            # print(paragraph)\n",
    "\n",
    "            sentences = []\n",
    "            for sentence in paragraph:\n",
    "                sentence = sentence.replace(\"\\n\", \"\") # remove the extra newlines between sentences\n",
    "                # print(sentence)\n",
    "                sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
    "\n",
    "            sentences = [[string for string in sublist if string] for sublist in sentences] # Remove the empty elements in every sentences if any\n",
    "            sentences = [string for string in sentences if string] # remove empty lists if any\n",
    "            # sentences.pop() # Drop the last two empty elements\n",
    "            end=0\n",
    "            for test in sentences:\n",
    "                if ('***END' in test) or (('END' in test)&('***' in test)):\n",
    "                    break\n",
    "                end=end+1\n",
    "\n",
    "            if end!=len(sentences):\n",
    "                del sentences[end:]\n",
    "\n",
    "            start=0\n",
    "            for test in sentences:\n",
    "                if ('***START' in test )or (('START' in test)&('***' in test)):\n",
    "                    break\n",
    "                start=start+1\n",
    "            if start!=len(sentences): \n",
    "                del sentences[:start+1]\n",
    "\n",
    "            return sentences\n",
    "\n",
    "        book=read_article(input_file)\n",
    "\n",
    "        def split(input_book):                   # chapterize the book\n",
    "\n",
    "            cnt=0\n",
    "            idx=dict()\n",
    "\n",
    "            for i in input_book:\n",
    "                if ('chapter' in map(str.lower, i[:1])) & (len(i)>=2) :\n",
    "                    idx[tuple(i)]=cnt\n",
    "\n",
    "                elif ('chapitre' in map(str.lower, i[:1])) & (len(i)>=2) :\n",
    "                    idx[tuple(i)]=cnt   \n",
    "\n",
    "                elif ('chap' in map(str.lower, i[:1])):\n",
    "                    idx[tuple(i)]=cnt\n",
    "\n",
    "                elif ('section' in map(str.lower, i[:1])):\n",
    "                    idx[tuple(i)]=cnt    \n",
    "\n",
    "                cnt=cnt+1\n",
    "\n",
    "            page=sorted(idx.values())\n",
    "\n",
    "            chapter=dict()\n",
    "            cnt=0\n",
    "            while cnt <=len(page)-1:\n",
    "\n",
    "                if cnt!=len(page)-1:\n",
    "                    chapter[cnt+1]=input_book[page[cnt]:page[cnt+1]]\n",
    "\n",
    "                else:\n",
    "                    chapter[cnt+1]=input_book[page[cnt]:]\n",
    "                cnt=cnt+1\n",
    "            return chapter\n",
    "\n",
    "        chap=split(book)\n",
    "\n",
    "        if chap =={}:                                 # backup chapterize method if split function fail\n",
    "            point=1\n",
    "            for page in range(300,len(book),300):\n",
    "                if (page+300)>len(book):\n",
    "                    chap[point]=book[page-300:page]\n",
    "                    chap[point+1]=book[page:]\n",
    "                else:\n",
    "                    chap[point]=book[page-300:page]\n",
    "                point=point+1\n",
    "\n",
    "        def chap_string(input_para):           # convert chap into string\n",
    "            string=[]\n",
    "            for tok in input_para:\n",
    "                string.append(\" \".join(tok))\n",
    "            string='. '.join(string)\n",
    "            return(string)\n",
    "\n",
    "        def agg_sum(input_sum):                    # convert individual output sentences from Sumy into one-paragraph summary\n",
    "            in_sum=[]\n",
    "            for sens in input_sum:\n",
    "                in_sum.append(str(sens))\n",
    "            in_sum=' '.join(in_sum)\n",
    "            return(in_sum)\n",
    "\n",
    "\n",
    "        plots=dict()\n",
    "\n",
    "        tf_sum=dict()\n",
    "        tf_len=dict()\n",
    "        for i in chap:\n",
    "\n",
    "            scores=[]\n",
    "\n",
    "            chap_sum=[]\n",
    "            for j in chap[i]:\n",
    "                chap_sum.append(\" \".join(j))\n",
    "            chap_sum=' '.join(chap_sum) \n",
    "\n",
    "            if len(chap[i])>500 :\n",
    "                continue\n",
    "            for k in range(1,min(30,len(chap[i])),1):       \n",
    "\n",
    "\n",
    "\n",
    "                parser = PlaintextParser.from_string(chap_string(chap[i]),Tokenizer(\"english\"))\n",
    "                summarizer = LexRankSummarizer()           \n",
    "                summary = summarizer(parser.document,k)      # find top 30 important sentence\n",
    " \n",
    "                data=[agg_sum(summary), chap_sum]            #record tf-idf info\n",
    "                vectorizer = TfidfVectorizer( )\n",
    "                vector1 = vectorizer.fit_transform(data)\n",
    "                tf_sum[i]=sum(vector1.toarray()[1])\n",
    "                tf_len[i]=len(vector1.toarray()[1])\n",
    "                sim=cosine_similarity(vector1)[1][0]\n",
    "                scores.append(sim)\n",
    "            plots[i]=scores.copy()\n",
    " \n",
    "        cut=dict()                                          # choose optimal cut\n",
    "        for i in plots: \n",
    "            pos=len(plots[i])\n",
    "            cut[i]=1\n",
    "            for sim in plots[i][::-1]:\n",
    "                if sim<=0.8*max(plots[i]):\n",
    "                    cut[i]=pos\n",
    "                    break\n",
    "                pos=pos-1\n",
    "\n",
    "        for indx in tf_sum:\n",
    "            df2 = {'tf_sum': tf_sum[indx], 'tf_len': tf_len[indx], 'optimal_cut': cut[indx]}\n",
    "            result = result.append(df2, ignore_index = True)\n",
    "        done.append(books)\n",
    "    except:\n",
    "        wrong.append(books)\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56039c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
